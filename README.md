g# Crisis Assistant: Offline, Human-Toned, RAG + Decision Tree Hybrid

An offline, voice-interactive assistant for emergencies/disasters. Human-like tone, Alexa-style interface, with deterministic decision trees for safety, augmented by RAG, and generated by a quantized local LLM. Target hardware: Raspberry Pi 5 (8GB). MVP runs on a laptop.

Features:
- Voice activated (wake word or button) → MVP includes text mode, voice integration hooks included.
- Offline STT: whisper.cpp wrapper
- Decision Trees: YAML, deterministic, safety-first overrides for high-risk scenarios (bleeding, CPR, burns)
- RAG: FAISS vector DB (with local sentence-transformers embedding), doc filters per node
- Offline LLM: llama.cpp via llama-cpp-python (Phi-3 Mini / TinyLlama GGUF)
- TTS: Piper wrapper
- Hybrid logic: Decision node can include rag_docs/tags → LLM wraps into empathetic, stepwise response

Safety principle:
- Decision trees always override LLM for critical steps.
- RAG augments critical guidance; LLM wraps into empathetic, stepwise, concise, human-like output.

Quick start (MVP, text mode):
1) Install dependencies
2) Put manuals into data/manuals/
3) Build local RAG index
4) Run the assistant

Commands:
- make venv
- make install
- python scripts/build_index.py --input data/manuals --out data/index
- python main.py

Example:
> You: Someone is bleeding badly!
> Assistant: I’m here to help. Apply firm, direct pressure to the wound immediately... (with WHO guidance if indexed)

Pi 5 guidance:
- Use quantized models (GGUF, 4/8-bit)
- Keep index small and curated
- Use whisper.cpp tiny/small; Piper with a light voice
- Expect ~4–8s latency end-to-end

Disclaimer:
- This is an educational prototype and not a substitute for professional medical advice. Always call emergency services where available.

## Structure

- configs/config.yaml: model paths, languages, toggles
- decision_trees/*.yaml: critical flows (bleeding, CPR, burns...)
- data/manuals/: PDFs or text; ingest to index
- data/index/: FAISS index + metadata
- src/: orchestrator, decision engine, retriever, llm engine, audio wrappers
- scripts/build_index.py: ingest manuals and build FAISS index
- systemd/: sample service for Pi autostart

## Roadmap

Phase 1 – Laptop MVP (this repo)
- Text mode interface
- Decision Tree + RAG + LLM hybrid responses
- Empathetic, stepwise tone

Phase 2 – Pi 5 Prototype
- Wire in whisper.cpp and Piper CLI
- Mic/speaker I/O; optional button/wake-word
- Optimize models and index size

Phase 3 – Field Ready
- Ruggedized hardware
- Multilingual
- Advanced logging, stress testing

## Setup

Python 3.10+ recommended.

1) Create venv and install:
- make venv
- make install

2) Place manuals:
- Put .pdf/.txt into data/manuals/
- Example provided: data/manuals/demo.txt

3) Build index:
- python scripts/build_index.py --input data/manuals --out data/index

4) Configure models in configs/config.yaml:
- Point model_path to your GGUF LLM (TinyLlama/Phi-3 mini)
- Configure Piper and whisper.cpp binary paths if using voice

5) Run MVP (text):
- python main.py

Optional: Systemd on Pi 5:
- Edit systemd/crisis-assistant.service ExecStart path
- sudo cp systemd/crisis-assistant.service /etc/systemd/system/
- sudo systemctl enable --now crisis-assistant

## Notes

- Offline embeddings: sentence-transformers model BAAI/bge-small-en-v1.5 (download once; then keep offline)
- FAISS may require aarch64; if unavailable, TF-IDF fallback is included
- Whisper/Piper wrappers call CLI binaries (install separately)
- Keep context small to reduce latency; use concise, empathetic outputs

## License
Choose and add a LICENSE as appropriate for your project.# SreRAG-2.0
# SreRAG-2.0
# SreRAG-2.0
# SreRAG-2.0
# SreRAG-2.0
